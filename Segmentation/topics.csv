Topic,Count,Name,Representation,Aspect1,Representative_Docs
0,2,0_Brightness and Illumination,['Brightness and Illumination'],"['illumination', 'brightness measurement', 'brightness', 'light', 'sun', 'surface', 'energy', 'talk brightness', 'angle', 'reflects']","[""But actually, all my vectors are supposed to be column vectors, so it's stuck in the transpose. So another bit of notation. So all pretty straightforward now. OK, let's talk about brightness. So brightness depends on a bunch of different things. It depends on illumination. And in a linear way, in that if you throw more illumination on an object, it's going to be brighter. And, you know, there are a few laws that are really, really linear. This is linear over many, many, many orders of magnitude. I mean, when does it stop being linear? Well, when you put so much energy on the surface that you're melting it. You know, you have to actually have enough energy to fry it. And it's a little bit like Ohm's law, which is also one of those remarkable things that, for some materials, is linear over many, many orders of magnitude. Anyway, so brightness depends on illumination."", ""And I can measure the sun is about a kilowatt per square meter. But obviously that same energy is spread out over a larger area, right? This, this. length is bigger than that length. And so the illumination of this surface is less. And how much? Well, we can express it in terms of this angle, which is the incident angle, theta of i. And that is the same angle as that angle, I think. And there's a cosine relationship between this red length and this length. So we find out that, in this case, the illumination on the surface varies as the cosine of the angle. And this is something that we'll see again and again. Now, it doesn't necessarily mean that its brightness, the amount of light it reflects, goes as the cosine of the incident angle. That is the simplest case. And so here's an example where we could use an image brightness measurement to learn something about the surface, because we can look at different parts of the surface, and they'll have different brightnesses depending on this angle.""]"
1,1,1_Course Structure Overview,['Course Structure Overview'],"['homework problems', 'problems quizzes', 'homework', 'chapters', 'problems', 'homework problem', 'term project', 'quizzes', 'machine vision', 'machine']","[""So, welcome to Machine Vision 6801-6866. And I'm not sure how we got so lucky, but we have the classroom that's the furthest from my office. So, I guess I'm going to get a lot of exercise, and I think we're going to have a lot of stragglers coming in. What's there to know? Just about everything's on the website, so I can probably eliminate a lot of the administrivia. Please make sure that you're actually registered for the course on the website, and take a look at the assignments. And hopefully, you've either had a chance to look at chapters one and two, or you're about to. That's the assignment for this week. And there's a homework problem. And you're probably saying, God, I just arrived here. How can there be a homework problem? Well, I'm sorry, but the term is getting shorter and shorter. And if I work backwards from when the faculty rules say the last assignment can be due, we have to start now. Now, the good news in return is there's no final. So yes, there's a homework problem starting right away, but there's no final. And there's a homework problem only every second week, so it's not a huge burden. And there are some take-home quizzes. So, two of the times where you'd normally have a homework are going to be sort of glorified homeworks that count more than the others. And they'll be called quizzes. So, total there, I think there are five homework problems and two quizzes. Collaboration, collaboration's okay on the homework problems, but please make a note of who you work with. It's not okay on the take-home quizzes. 6.866, so those of you in 6.866, the difference is that there's a term project. So, you'll be implementing some machine vision method, preferably one that we cover in the course. And there'll be a proposal due about a month from now. I'll let you know as we go along, tell me what you're planning to do. And, you know, preference is going to be given to sort of dynamic problems rather than single image static analysis, image motion, that kind of thing. And if there's enough interest, we'll have a session on how to do this on an Android phone.""]"
2,1,2_Android project development,['Android project development'],"['android', 'android phone', 'camera', 'term project', 'phone', 'project', 'term', 'don', 'years', 'doing']","[""And I'm a little reluctant to do that because some of you don't have an Android phone. And I have some loaners, but you know what it's like. These darn things go out of fashion in two years. And so, all of the interesting new stuff having to do with a camera on Android is not in some of the box full of old smartphones I have. But that is an option. So, one of the ways of doing your term project is to do an Android Studio project.""]"
3,1,3_3D to 2D mapping,['3D to 2D mapping'],"['surface image', 'tilt surface', 'reflectance', 'surfaces', 'surface', '2d images', '3d', 'retina', 'tilt', '2d 2d']","[""And as I tilt the surface more and more and more, that image area becomes smaller and smaller. But it's receiving the same power, supposedly, according to these guys. And what does that mean? That means you're going to fly your retina right at the occluding boundary. Because all that energy is now focused in the tiny, tiny area. So this is an important idea. And it comes in when we talk about the reflectance of surfaces. And we need to be aware of it. Now, something I want to end up on is we're solving a tough problem with 3D. We only got 2D images. So maybe we're lucky and we have several. But you've got a function of three variables that's got so much more flexibility than a few functions of two variables. So why does this work at all? Well, the reason it works is that we're not living in a world of color jello. So we're living in a very special visual world. So if I'm looking at some person back there, the ray coming from the surface of his skin to my pupil is not interrupted. And it's a straight line. Why? Well, because we're going through air. And air is refractive index almost exactly 1. And at least it doesn't vary from that position to here. And there's nothing in between. There's no smoking allowed in this room. So it can't be absorbed. And that's very unusual. And the other thing is that that person has a solid surface. I'm not looking into some sort of semi-translucent, complicated thing. So they're straight line rays. And there's a solid surface. Therefore, there's a 2D to 2D correspondence. The surface of that person. Sorry, I keep on looking at the same person. It's getting embarrassed. But 2D, we can talk about points on the cheek of this person using two vectors, u and v. And that's mapped in some curvilinear way into the 2D in my image. And that's one reason why this works. It's not really 3D to 2D.""]"
4,1,4_Light and Imaging,['Light and Imaging'],"['illumination', 'light sources', 'imaging', 'inversion', 'tilt surface', 'rays', 'surface', 'light', '3d', 'type surface']","[""Or you can use colored light sources, and so on. But suppose you were interested in, how come people can do this? They don't play tricks with light sources. And they don't live in a world with three suns of different colors. Then we'll have to do something more sophisticated. And we'll study that. OK, how are we doing? Are we getting there? OK, so the foreshortening comes up in two places. The one is here, where we're talking about incident light. But actually, foreshortening also plays a role in the other direction. Whoa, high friction blackboard. Also hard to erase. OK, so. So it's really the same geometry, except now the rays are going in the other direction. And so. And I have a foreshortening on the receiving end as well. So in a real imaging situation in 3D, we'll see both of these phenomena. There's the foreshortening that affects the incident illumination as up there. And then there's this effect. And for example, I can illustrate to you right away the stupidity of some textbooks. So some textbooks say that there's a type of surface called inversion, which emits energy equally in all directions. That's what they literally say. Well, if that's true, then that energy is imaged in a certain area that changes as I change the tilt of the surface.""]"
5,1,5_Surface Orientation Geometry,['Surface Orientation Geometry'],"['surface orientation', 'surface normal', 'orientation', 'surface', 'sphere', 'constraints', 'constraint', 'unit vector', 'angle', 'vector']","[""And the way I can talk about the orientation of the surface is just to tell you what that unit normal is. So how many degrees of freedom are there? How many numbers do I need? Well, I need three numbers to define a vector. So it sounds like three, except I have a constraint, right? Three components. Like, this isn't just any old vector. This is a unit vector. So x squared plus y squared plus z squared equals to 1. So I have one constraint. So actually, surface orientation has two degrees of freedom. And since this is such an important point, let's look at it another way. So another way of specifying surface orientation is to take this unit normal and put its tail at the center of a unit sphere and see where it hits the sphere. And so every surface normal has two degrees of freedom. And so every surface orientation, then, corresponds to a point on the sphere. And I can talk about points on the sphere using various ways. But one is latitude and longitude. And that's two variables. So that tells us, again, in another way that a unit normal has two degrees of freedom. And if I want to pin it down, I better have two constraints. So that's where that was going. And that makes it interesting. I mean, if we could just say, OK, I measure the brightness, and it's 0.6, and the orientation is such and such, the course would be over. It'd be pretty boring. But it isn't. It's not easy. We need more constraint. And we'll see the different ways of solving this problem. One of them you saw in the slides was a kind of brute force one, saying, well, we just get more constraint. We illuminate it with a different light source. We get a different constraint, because the other light source will have a different angle. And then I can solve at every point. So from an industrial implementation point of view, that's great. You can do that. You can either use multiple light sources, put on at different times.""]"
6,1,6_Vector Notation and Projection,['Vector Notation and Projection'],"['perspective projection', 'vector', 'vector notation', 'projection', 'optical axis', 'vectors', 'unit vector', 'define vector', 'components', 'component']","[""The second component is little y over f is big y over big z. And the third component is f over f is z over z, so that doesn't do anything to us. So that's the equivalent, and now I can just define a vector r. So this is r, little r. And now I've got sort of a mixed notation, right? Because I've got a big z in here. Well, that's the third component of big r vector. So I just dot product with unit vector z. So let me write that out in full. So that's x, y, z dot. So the unit vector in the z direction along the optical axis is just 0, 0, 1, right? And so I finally have the equivalent of the equations up there in component form. I have it here in vector form. So that's perspective projection in vector form. Now, usually at this point, you say, look how easy it got by using vector notation. Well, it isn't really any easier looking. This is one of those rare cases where it didn't buy you a whole lot in terms of number of symbols you have to write down and so on. Nevertheless, the compactness of that notation comes out when we start manipulating it. If you have to carry around all these individual components all the time, that can get pretty tedious, whereas if you use the vector, it's more interesting. And as I've mentioned, one of the things we're going to do soon is differentiate that with respect to time. And then on the left, we'll have image motion, and on the right, we'll have real world motion. And the equation we get will give the relationship between the two. Okay. So this may sound a little bit haphazard and chopped up, what we're doing today. And that's only because I want to cover stuff in chapter 1 and 2 and material you need for the homework problem. So we're going to, rather than pursue perspective projection more, we're going to jump to brightness in a second. But first, let me say something else, which is that I'm thinking of these vectors as column vectors. You know, and that's sort of arbitrary because we can establish a relationship between skinny matrices and vectors either way, right? I can think of, you know, x, y, and z stacked up vertically above each other as a 3 by 1 matrix, or I can write them horizontally, x, y, z, and it's a 1 by 3 matrix. And just for consistency, I'm always going to think of them as column vectors. And that's why sometimes I need a transpose. And that's what the symbol t is for. So I didn't say it here, but we can now go back to this. So if I write it this way, it's a row vector.""]"
7,1,7_3D Motion and Vectors,['3D Motion and Vectors'],"['3d motion', '3d', 'motion', 'using vectors', 'vectors', 'vector', 'vector notation', 'components', 'unit vector', '2d']","[""And when we differentiate, we can get a relationship between motion in 3D and motion in the image. And why is that interesting? Well, it means that if I can measure motion in the image, which I can, I can try and guess what the motion is in 3D. Now, the relationship's not that simple. For example, if the motion in 3D is straight towards me, the baseball bat's going to hit me in the head, then the motion in the image is very, very small. So I have to take into account that transformation. But I do want to know that relationship between motion in 3D and motion in 2D, and I get it just by differentiating that. Then I want to introduce several things that we'll use a lot in the course. The next one is vectors. So we're in 3D. Why am I talking about components? I should be just using vectors. So first of all, notation. In publications, in engineering publication, not math publication, vectors are usually denoted with bold letters. And so if you look at the robot vision or some paper on the subject, you'll see vectors in bold. Now, I can't do bold on the blackboard, and so we use underline. And actually, there was a time where you didn't typeset your own papers. Just a second. But somebody at the publishers typeset your paper. So how did you tell them to typeset in bold? You underlined it. I mean, the camera actually works the way that works up there in most cases. Some of them will have mirrors to fold the optical. path. This is like a conceptual convenience just to make it easier not to have to, I mean maybe some people don't have a problem with minus signs, but to me it's confusing having that one upside down, so I prefer to do it this way. But the actual apparatus works that way. Okay, so the other bit of notation we need is a hat for unit vector because we'll be dealing with unit vectors quite a bit. For example, you saw that we talked about the surface orientation on that donut in terms of unit vectors. It's a direction, so we'll use a hat on top of the vector. And okay, so let's turn that into vector notation. Well, how about this? So I claim that this is basically the same as that up there, right? Because if you go component by component, the first component is little x over f is big x over big z.""]"
8,1,8_Coordinate systems and motion,['Coordinate systems and motion'],"['image plane', 'coordinate robot', 'camera', '3d', 'transformation', 'points image', 'axis', 'retina', 'upside', 'motion']","[""That's going to be our z-axis. So it's a very special coordinate system. But it makes the whole thing very easy. And then if we do have a different coordinate system on our robot or whatever, we just need to deal with the transformation between this special camera-centric coordinate system and that coordinate system. Now, one of the things that's very convenient, well, not only are they going to make me walk all across campus, but I'm going to get upper body strength as well. This is great. OK, so what we do is we flip the image plane forward. So you know, the image on your retina is upside down. And in many cases, that's sort of inconvenient. And so what we can do is we can just pretend that the world actually looks like this, right? That's pretty much the same diagram. We've just flipped 180 degrees what was behind the camera and in front. And it makes the equations even more obvious, right? The ratio of this to that is the ratio of this to that. Now, that sounds kind of straightforward and somewhat boring, but it has a number of implications. The first one is it's nonlinear, right? So we know that if things are linear, our math becomes easier and so on. But here, we're dividing by z. So on the one hand, that's an inconvenience because like you take derivatives and stuff or the ratio, that's not so nice. But on the other hand, it gives us some hope because if the result depends on z, we can turn that on its head and say, oh, maybe then we can find z, right? So we can get an advantage out of what seems like a disadvantage. And then the next thing is, we won't do it today, but we'll be doing it soon, is to talk about motion. So what happens? Well, we have a motion. So what happens? Well, we just differentiate that equation with respect to time. And what will that give us? Right now, we have a relationship between points in 3D and points in the image.""]"
9,1,9_Machine Vision Fundamentals,['Machine Vision Fundamentals'],"['camera', 'surface orientation', 'focus', 'image plane', 'images', 'lens', 'surface', 'orientation', 'image', 'graphics']","[""If your machine vision program is not working, that probably won't happen. So in many other cases, if the final output is a description of the environment, who's to say whether it's correct? It depends on the application. I mean, if it's there for purposes of writing a poem about the environment, that's one thing. If there's purposes to assemble an engine, then it's this type of situation where we have some feedback. If it works, then probably the machine vision part worked correctly. Here's the time to contact problem that I was talking about. And as you can imagine, of course, as you move towards the surface, the image seems to expand. And that's the cue. But how do you measure that expansion? Because all you've got are these gray levels, this array of numbers. How do you measure that? And how do you do it accurately and fast? And also, we've noted that somehow there are interesting aspects, like one camera, don't need two. The other one is that for many of the things we do, we need to know things about the camera, like the focal length. And we need to know where the optical axis strikes the image plane. So we've got this array of pixels. But where's the center? Well, you can just divide the number of columns and the number of rows by two. But that's totally arbitrary. What you really want to know is, if you put the axis through the lens, where does it hit that image plane? And of course, the manufacturer typically tries to make that be exactly the center of your image sensor. But it's always going to be a little bit off. And in fact, in many cases, they don't particularly care. Because if my camera puts the center of the image 100 pixels to the right, I probably won't notice. In normal use, if I'm going to post on Facebook, it doesn't really make any difference. If I'm going to use it in industrial machine vision, it does make a difference. And so that kind of calibration is something we'll talk about as well. And what's interesting is that in this particular case, we don't even need that. We don't even need to know the focal length, which seems really strange. Because if you have a longer focal length, that means the image is going to be expanded. So it would seem that that would affect this process. But what's interesting is that at the same time as the image is expanded, the image motion is expanded. And so the ratio of the two is maintained. So from that point of view, it's a very interesting problem. Because unlike many others, we don't need that information. So here's an example of approaching this truck. And over here is a plot, time, horizontal. And vertical is the computed time to contact. The red curve is the computed. And the barely visible green dotted line is the true value. In the process, by the way, we expose another concept, which is the focus of expansion. So as we approach this truck, you'll notice that we end up on the door, which is not the center of the first image. So we're actually moving at an angle. We're not moving straight along the optical axis of the camera, but we're moving at an angle. And the focus of expansion is very important, because it tells us in 3D what the motion vector is. So in addition to finding the time to contact, we want to find the focus of expansion. And there's another one. This one was done using a time lapse, moving the car a little bit every time. And well, I'm not very good at moving things exactly. with 10 millimeters, so it's a bit more noisy than the previous one. So yeah, we'll be talking a little bit about coordinate systems and transformations between coordinate systems. For example, in the case of robot applications, we'll want to have a transformation between a coordinate system that's native to the camera to a coordinate system that, you know, when you get the robot, it has kinematics programmed into it so that you can tell it in X, Y, Z where to go and in angle how to orient the gripper, but that's in terms of its defined coordinate system, which is probably, you know, the origins in the base where it's bolted in the ground, whereas your camera up here, it probably likes a coordinate system where its center of projection is the origin. So we'll have to talk about those kinds of things. And I won't go into that. We'll talk about this later. So I mentioned analog computing. And, you know, now we just automatically everything is digital, but there are some things that are kind of tedious. You know, if you have to process 10 million pixels and do complicated things with them, that can't, since digital computing isn't getting any faster, that can be a problem. Okay, so you can use parallelism and, you know, so. So there's still an interest in analog. And so here, this is the output of a chip that we built to find the focus of expansion. So, and it's basically instantaneous, unlike the digital calculation. And the plot is a little hard to see, but let's see the circles. The circle, they're determined by two different algorithms. And you can see that, you know, there's some error, but overall the cross, the X and the O are sort of on top of each other. So this was a fun project because, you know, to have a chip fabricated is expensive. And so you can't afford to screw up too many times. And of course, with an algorithm this complicated, you know, what's the chance you'll get it right the first time. So the student finally reached the point where opera wouldn't pay for any more fabs. And the last problem was there was a large current to the substrate, which caused it to get warm. And of course, once it gets hot, it doesn't work anymore. So he'd come in every morning with a cooler full of ice cubes and a little aquarium pump and cool his focus of expansion chip to make sure that it wouldn't overheat. So we talked a little bit about projection and motion. Let's talk about brightness. So as you'll see, you can kind of split down the middle what we'll have to say about image formation. So the first half is the one that's, you know, covered in physics, projection. It answers the question where. So what is the relationship between points in the environment and points in the image? And well, rays, you know, you connect it with a straight line through the center of projection and you're pretty much done. That's called perspective projection. And we'll talk about that. But then the other part of half of the question is how bright, you know, what is the gray level at a point or in color terms, RGB values at a point. And so that's less often addressed in some other courses. And we'll spend some time on that. And obviously, we'll need to do that if we're going to solve that shape from shading problem, for example. So this is, what is this? So we've got three pictures here taken from pretty much the same camera orientation and position of downtown Montreal. And obviously, if you go to a particular pixel in the three images, they're going to have different values, right? Because the lighting's changed. So what this illustrates right away is that illumination plays an important role. And obviously, we'd like to be insensitive to that. And in fact, you know, if you showed anyone, anyone of these three pictures separately, they say, oh, yeah, OK, that's plus Saint-Ville-Marie. And, you know, they wouldn't even think about the fact that the gray levels are totally different because we automatically accommodate that difference. So we'll be looking at diagrams like this where we have a light source shown as the sun and an image device shown as an eye and a tiny piece of the surface and the three angles that control reflection. And so what we see from that direction is a function of where that light comes from, what type of a material it is, and how it's oriented. And we'll particularly focus on that orientation question because if we can figure out what the surface orientation is at lots of points, we can kind of try and reconstruct the surface. And you know, there's that business of counting constraints, again, because the what's the surface orientation? It's two variables, right? Because you can tilt it in X and you can tilt it in Y. That's sort of the crude way to see why that is. And what are we getting? We're getting one brightness measurement. So we're kind of it's not clear you can do it. It might be under constraint. And the image you get of an object depends on its orientation. And the way I've shown it here is to show the same object basically in many different orientations. And not only does it outline change, but you can see the brightness within the outline depends a lot on that as well. And things depend a lot on surface reflecting properties. So on the left, we have a matte surface, you know, white matte paint out of a spray can. And on the right, we have a metallic surface. And so even though it's the same shape, we have a very different appearance. And so we'll have to take that into account and try and understand how do you describe that? You know, what equation or what terminology shall we use for that? So we'll sort of jump ahead here to one approach to this question, which is, you know, suppose we lived in a solar system with three suns that have different colors. No. This is what we get. There's a cube. And it will make things very easy, right? Because there's a relationship between the color and orientation. So if I have that particular type of blue up there, I know that the surface is oriented in that particular way. So that would make the problem very easy. And so that leads us to an idea of how to solve this problem. So as I mentioned, there's this so-called bin of parts problems, which we were foolishly enough to believe what the mechanical engineers wrote in the annual report. So what they said was here are the 10 most important problems to solve in mechanical engineering. And this was, I forget, number two. You know, how to pick parts when they're not palletized, when they're not perfectly arranged. And so here, the task is to take one after another of these rings off the pile of rings. And of course, because they're lying, if they were just lying on the surface, it would be easy because there are only that many stable positions. Well, for this object, there are only two. And so it would be pretty straightforward. But since they can lie on top of each other, they can take on any orientation in space. And also, they obscure each other. And also, shadows of one fall on the other. So it gets more interesting. And you can see that it took many experiments to get this right. So these objects got a little bit hammered. So you have to be insensitive to the noise due to that. And we need a calibration. So we need to know the relationship between surface orientation and what we get in the image. And so how best to calibrate? Well, you want an object of known shape. And nothing better than a sphere for that. It's very cheap. You just go to the store and buy one. You don't have to manufacture a paraboloid or something. And this may be a little odd picture. But now, this is looking up into the ceiling. So in the ceiling, there are three sets of fluorescent lights. And in this case, they're all three turned on. But in the experiment, they're used one at a time. So you have three different illuminating conditions. And we get a constraint at each pixel out of each one. So ta-da, we have enough constraints, right? We've got three constraints at every pixel. We need two for surface orientation. And we have an extra one. Well, the extra one allows us to cope with albedo, changes in reflectance. So we can actually recover both the surface orientation and the reflectance of the surface if we do this with three lights. So here's our calibration object illuminated by one of those lights. And now, we repeat it with the other two. And just for human consumption, we can combine the results into an RGB picture. So this is actually three separate pictures. And we've used them as the red, green, and blue planes of a color picture. And you can see that different surface orientations produce different colors, meaning different results under the three illuminating conditions. And so conversely, if I have the three images, I can go to a pixel, read off the three values, and figure out what the orientation is. And you might see a few things. One of them is that there's certain areas where the color is not changing very rapidly. Well, that's bad, right? Because that means that if there's some small error in your measurement, you can't be sure exactly where you are. And the other areas where the color is changing pretty dramatically, and that's great because any tiny change in surface orientation will have an effect. And so one of the things we'll talk about is that kind of noise gain, that sensitivity to measurement error. Why worry about it? Well, images are noisy. So first of all, a lot of the images you're looking at, they're 8-bit images. That's one part in 256. You know, that's really crude quantization. And you can't even trust the bottom one or two bits of those. If you're lucky and you get raw images out of a fancy DSLR, you might have 10 bits or 12. Another way to look at it is that a pixel is small. How big is a pixel in a typical camera? So we can sort of figure it out. So the chip is, you know, a few millimeters by a few millimeters, and we got a few thousand by a few thousand columns and rows. So it's sort of a few microns. And, you know, there are huge trade-offs, like the one in your phone has smaller pixels. The one in a DSLR has larger pixels. But in any case, they're tiny. Now imagine that there's light bouncing around the room. A little bit of that light goes through the lens, and a tiny, tiny part of that gets onto that one pixel. So the number of photons that actually hit a pixel is relatively small. It's like a million or less. And so that means that now we have to worry about statistics of counting. As you can imagine, if you have 10 photons, is it nine, is it 10, is it 11? You know, that's a huge error. So if you're a million, it's already better. It's like one in a thousand. So the number of photons that can go into a single pixel is small. But not only is there little light coming in, but actually the pixel itself can't store that much. The photons are converted to electrons. Each pixel is like a tiny capacitor that can take a certain charge before it's full. So anyway, images are noisy, so we have to be cognizant of that. So that was the calibration. Now we go to the real object. And again, different surface orientations produce different colors. From that, we can construct this so-called needle diagram. So imagine that we divide the surface up into little patches, and at each point, we erect the surface normal. And then these tiny little, maybe hard to see, but they're tiny little bluish spikes that are the projections of those surface normals. So in some areas, like here, they're pretty much pointing straight out at you. So here you're looking perpendicular onto the surface, whereas over here, the surface is curving down and you're looking sideways. So that's a description of the surface, and we could use that to reconstruct the shape. But if we're doing recognition and finding out orientation, we might do something else. So here, you see it's actually slightly more complicated because you've got shadows and it's harder to see, but there's also interflection. That is, with these white objects, light bounces off each of them in a matte way, goes everywhere, and it spills onto the other surfaces. So it's not quite as simple as I explained. So what do we do with our surface normals? We want a compact, convenient description of shape. And for this purpose, one such description is something called an extended Gaussian image, which we'll discuss in class, where you take all of those needles and you throw them out onto a sphere. And so, for example, for this object, we have a flat surface at the top. All of those patches of that surface have the same orientation, so they're gonna contribute that big pile of dots at the North Pole. So just cut that short, it's a representation in 3D that's very convenient if we need to know the orientation of the object. Because if we rotate this object, that representation just rotates. You can think of many other representations that don't have that property. Okay, so here it is. You could imagine that it wasn't easy to get the sponsor of the project to pay for these parts here. I think they were concerned they were not for experimental purposes. So this is a single camera system, so there's no depth. So the way this works is that you do all this image processing, you figure out which object to pick up and how it's oriented, and then you reach down with a hand until a beam is interrupted. Then you know the depth. So here the beam is interrupted, and now the robot backs up and re-orients the hand for grasping. And then it comes back and grasps that object and so on. And I show this because another calibration I left out was what I previously mentioned, the relationship between the robot coordinate system and the vision system coordinate system. And one way of dealing with that is to have the robot carry around something that's easy to see and accurately locatable. This is something called a surveyor's mark because surveyors have used that trick for a very long time. It's easy to process the image, and you can find the location or the intersection of these two lines very accurately with sub-pixel accuracy. So you move that around in the workspace and then fit the transformation to it, and then you can use that to... Okay, back to more serious stuff. So that should give you a sort of taste of the kind of thing that we'll be doing. And what I'm going to do now is work towards what you need for the homework problem. So first, are there any questions about what you saw? I mean, a lot of that's gonna get filled in as we go through the term. Okay, so I mentioned this idea of inverse graphics. So if we have a world model, we can make an image. And people who are into graphics will hate me saying that, but that's the easy part. That's the forward problem, right? It's well-defined. And the interesting part is, how do you do it well? How do you do it fast? How do you do it when the scene has only changed slightly and you don't want to have to recompute everything and so on? But what we're trying to do is kind of invert that process. So we take the image, and we're trying to learn something about the world model. And we're trying to learn something about the world. Now, we can't actually reconstruct the world. We typically don't end up with a 3D printer doing that. Usually, this ends at a kind of description. It might be a shape or identity of some object or its orientation in space. Whatever is required for the task that we have. It might be some industrial assembly task or it might be reading the print on a pharmaceutical bottle to make sure that it's readable and so on. But that's the kind of loop, and that's why we like to talk about it as inverse graphics. Now, to do that, we need to understand the image formation. And that sounds pretty straightforward, but it has two parts, both of which we'll explore in detail as we go along. Then, with inverse problems, here we're trying to invert that, we often find that they're ill-posed. And as I mentioned, that means that they don't have a solution, have an infinite number of solutions, or have solutions that depend sensitively on the data. And that doesn't mean it's hopeless, but it does mean that we need methods that can deal with that. And often we'll end up with some sort of optimization method. And in this course, the optimization method of choice is least squares. Why is that? Well, you know, the fancy probability people will tell you that this is not a robust method. If you have outliers, it won't work very well. And that's great. But in many practical cases, least squares is easy to implement and leads to a closed form solution. Wherever we can get a closed form solution, we're happy because we don't have iteration. We don't have the chance of getting stuck in a local minimum or something. So we'll be doing a lot of least squares. But we have to be aware of, I already mentioned, noise gain. So not only do we want to have a method for solving the problem, but we'd like to be able to say how robust it is. You know, if my image measurements are off by 1%, does that mean that the answers are completely meaningless or does it mean that they're just off by 1%? So that kind of thing. Okay. Diving right in, we're going to address this problem. And it's kind of straightforward. And we'll start off with something called the pinhole model. Now we know that, you know, real cameras use lenses or in some cases mirrors. Why pinholes? Well, that's because the projection in the camera with a lens is the same as, it's trying to be exactly the same as a pinhole camera. By the way, there's a great example of a pinhole camera in Santa Monica. It's a camera obscura. You walk into this small building that's completely windowless. It's dark inside and there's a single hole in the wall. And on the other side, on the other wall, painted white, you see an inverted image of the world, and you see people walking by and so on. So that's a nice example of a pinhole camera. So here's a box to keep the light out. And then we have a hole in it. And on the opposite side of the box, we see projected a world, a view of the world. So let's just try and figure out what that projection is. So we've got, so there's a point in the world, uppercase P, and there's a little P point in the image plane. So the back of the box is going to be our image plane. And, you know, our retina is not flat. We're just going to deal with flat image sensors because, you know, all the semiconductor sensors are flat. And if it's not flat, we can transform. So, but we'll just work with that. Okay, so what we want to know is what's the relationship between these two. And so this is a 3D picture. And now let me draw a 2D picture. Okay, so we're going to call this f. And f is alluding to focal length. Although in this case, there's no lens. There's no focal length. But we'll just call that distance f. And we'll call this distance little x. And we'll call this distance big x. And this distance big z. So in the real world, we have big x, big y, big z. And in the image plane, we have little x. And we're going to have little y and f. And, well, they're similar triangles. So we can immediately write, okay. And now, you know, although this isn't completely kosher, I can do the same thing in the y plane. So I can draw the same diagram, just slice the world in a different way. And I get the companion equation. And that's it. That's perspective projection. Now, why is it so simple? Well, it's because we picked a particular coordinate system.""]"
10,1,10_Machine vision applications,['Machine vision applications'],"['machine vision', 'speed', 'vision', 'images', 'objects', 'systems', 'methods', 'humans', 'orientation', 'surface']","[""And I need to estimate the speed. Both of these are machine vision problems that we can attack. And it turns out that there's a very direct method that doesn't involve any higher level reasoning that gives us that ratio. And it's very useful. And it's also suggestive of biological mechanisms. Because animals use time to contact for various purposes, like not running into each other. And they use them, you know, flies, pretty small nervous system, use time to contact to land. So they know what to do when they get close enough to the surface. And so, you know, it's interesting that we can have some idea about how a biological system might do that. Contour maps from aerial photographs, you know, that's how all maps are made these days. And we'll talk about some industrial machine vision work. And that's partly because, actually, those systems really have to work very, very well. Not like, you know, 99% of the time. And so they actually poo-poo some of the things as academics talk about, because they're just not ready for that kind of environment. And they've come up with some very good methods of their own. And so it'll be interesting to talk about that. So at a higher level, we want to develop a description of the environment just based on images. One of the... After we've done some preliminary work and put together some methods, we'll use them to solve what was at one point thought to be an important problem, which is picking an object out of a pile of objects. So in manufacturing, often parts are palletized or, you know, arranged. Resistors come on a tape. And so by the time they get to the machine that's supposed to insert them in the circuit board, you know its orientation. And so that makes it very simple to build advanced automation systems. But when you look at humans building things, you know, there's a box of this, and there's a box of that, and there's a box of these other types of parts. And they're all jumbled. And they don't lie in a fixed orientation so that you can just grab them. using fixed robotic motions. And so we will put together some machine vision methods that allow us to find out where a part is and how to control the manipulator to pick it up. We'll talk a lot about ill-posed problems.""]"
11,1,11_Motion vision estimation,['Motion vision estimation'],"['motion vision', 'vision problem', 'distance', 'motion', 'vision', 'speed', 'time contact', 'cameras', 'physics', 'depth']","[""It's a very simple motion vision problem. But it's a good place to start talking about motion vision. So as I mentioned, we will take a physics-based approach to the problem. And we'll do things like recover observer motion from time-varying images. Again, we can think of autonomous cars. We can recover the time to collision from a monocular image sequence. That's sort of interesting because you think that to get depth, we might use two cameras and binocular vision, like we have two eyes and a certain baseline. And we can triangulate and figure out how far things are away. And so it's kind of surprising that it's relatively straightforward to figure out the time to contact, which is the ratio of the speed to the distance. So if I've got 10 meters to that wall and I'm going 10 meters per second, I'll hit it in a second. So I need to do two things. I need to estimate the distance.""]"
12,1,12_3D vision and technology,['3D vision and technology'],"['3d', 'motion vision', 'optical mouse', 'vision', 'optical', 'vision problem', 'mouse', 'flat', 'motion', 'pretty']","[""And if we do get 3D, then it's usually not very great quality. But we know that humans find it pretty straightforward to see three-dimensional shapes that are depicted in photos. And photos are flat. So where does the 3D come from? So that's something we'll look at. Then there are really simple questions like, oh, I forgot my optical mouse. How do optical mice work? Well, it's a motion vision problem.""]"
13,1,13_Computational Imaging Techniques,['Computational Imaging Techniques'],"['lenses', 'computational imaging', 'lens', 'imaging', 'machine vision', 'human vision', 'tomography', '3d picture', 'cameras', 'camera']","[""Lenses are incredible. Lenses are analog computers that take light rays that come in and reprogram them to go in different directions to form an image. And they've been around a few hundred years. And we don't really appreciate them, because they do it at the speed of light. I mean, if you try to do that in a digital computer, it would be very, very hard. And we've perfected them to where, you know, I just saw an ad for a camera that had a 125 to 1 zoom ratio. I mean, if the people that started using lenses, like Galileo and people in the Netherlands, you know, they'd be just amazed at what we can do with lenses. So we have this physical apparatus that will do this kind of computation. But there are certain cases where we can't use that. So for example, in computed tomography, we're shooting x-rays through a body. We get an image. But it's hard to interpret. I mean, you can sometimes see tissue of very high contrast, like bones will stand out. But if you want a 3D picture of what's inside, you have to take lots of these pictures and combine them computationally. We don't have a physical apparatus like an x-ray, lens, mirror, gadget, interferometer that will, you know, the final result is the image. Here, the final result is computed. Even more so in MRI. You know, we have a big magnet with a gradient field. We have little magnets that modulate it. We have RF. Some signal comes out. It gets processed. And ta-da, we have an image of a cross-section of the body. So that's computational imaging. And we won't be doing that. There is a course, 6870, which is not offered this term. But it goes into that. And we're also not going to say much about human vision. Again, course 9 will do that. Now in the interest of getting far enough to do the homework problem, I was going to not do a slideshow. But I think it's just traditional to do a slideshow. So I will try and get this to work. It's not always successful because my computer has some interface problems. But let's see what we can do. Okay. So let's talk about machine vision and some of the examples you'll see in this set of slides. Not all of it will be clear with my brief introduction. But we'll go back to this later on in the term. So what are the sorts of things we might be interested in doing? Well, one is to recover image motion. And you can imagine various applications in, say, autonomous vehicles and what have you. Another thing we might want to do is estimate surface shape. As we said, we don't get 3D from our cameras, well, not most cameras.""]"
14,1,14_Machine Learning Overview,['Machine Learning Overview'],"['computational imaging', 'imaging', 'image formation', 'physics', 'computational', 'learning', 'computing', 'lenses', 'courses', 'physical apparatus']","[""And that brings me to machine learning. This is not a machine learning course. And there are 6.003, 6.006, 6.008, 6.009, 6.008, 6.002, 6.008, 6.007, et cetera, et cetera. So there are plenty of machine learning courses, and we don't have to touch on that here. And also, I want to kind of show how far you can get just understanding the physics of the situation and modeling it without any black box that you feed examples into. In other words, we're going to be very interested in so-called direct computations, where there's some simple computation that you perform all over the image, and it gives you some result, like, OK, my optical mouse is moving to the right by 0.1 centimeter or something like that. It's also not about computational imaging. And what is that about? So computational imaging is where image formation is not through a physical apparatus, but through computing. That sounds obvious. Well, we have lenses.""]"
15,1,15_Physics-based Machine Vision,['Physics-based Machine Vision'],"['understand image', 'textbook', 'courses', 'learning', 'vision', 'human vision', '3d', 'geometry', 'environment', 'description environment']","[""And so, the errata for the textbook are online. So, if you have the book, you could go through and read, mark all of the bad spots. So, reading, read chapters one and two. Don't worry about all the reference material. You won't be reading all of it. So, what are we doing today? Well, mostly, I need to tell you enough so you can do the homework problem. That's one function. And the other one is to give you an idea of what the course is about. These two things kind of conflict. So, I'll try and do both. In terms of the course, I am supposed to tell you what the objectives are. So, I made up something. Learn how to recover information about environment from the images. And so, we're going to take this sort of inverse graphics view where there's a 3D world out there. We get 2D images. And we're trying to interpret what's happening in the world. Vision is an amazing sense because it's non-contact. And it provides so much information. But it's in a kind of coded form because we're not getting all the information that's possible. We don't get 3D, for example. So, that's the topic that we're going to discuss. And hopefully, you will then understand image formation and understand how to reverse that to try and get a description of the environment from the images. Outcomes. Well, you understand what's now called physics-based machine vision. So, the approach we're going to take is pretty much, you know, they're light rays. They bounce off surfaces. They form an image. And that's physics. You know, rays, lenses, power per unit area, that kind of stuff. And from that, we can write down equations. We can see, you know, how much energy gets into this pixel in the camera based on the object out there, how it's illuminated, how it reflects light, and so on. And from the equations, we then try to invert this. So, the equations depend on parameters we're interested in, like speed, time until we run into a wall, the type of surface cover, and so on. So, that's physics-based machine vision. And it's preparation for more advanced machine vision courses. So, there's some basic material that everyone should know about how images are formed that's going to be useful for other courses. And if you're going into learning approaches, one of the advantages of taking this course is it'll teach you how to extract useful features. So, you know, you can learn with raw data, like just the gray levels at every pixel. And that's not a particularly good approach. It's much better if you can already extract information, like texture, distance, shape, size, and so on, and do the more advanced work on that. And, well, also, one of the things some people enjoy is to see real applications of some interesting but relatively simple math and physics. You know, it's like sometimes we forget about this when we're so immersed in programming in Java or something. But there's a lot of math we learned and sometimes resented learning, because, like, why am I learning this? Well, it's neat to find out that it's actually really useful. And so that brings me to the next topic, which is that, yes, there will be math, but nothing sophisticated. It's, you know, engineering math, calculus, that kind of thing, derivatives, vectors, matrices, maybe a little bit of linear algebra, maybe some ordinary differential equation. you know, that kind of stuff, nothing too advanced, no number theory or anything like that. And there'll be some geometry and a little bit of linear systems. So you saw the prerequisite was 6.003, and that's because we'll talk a little bit about convolution when we talk about image formation, but we're not going to go very deep into any of that. First of all, of course, it's covered in 6.003 now, since they changed the material to include images. And then we have other things to worry about. So that's what the course is about, and I should also tell you what it's not. So it's not image processing. So what's the difference? Well, image processing is where you take an image, you do something to it, and you have a new image, perhaps improved in some way, enhanced edges, reduced the noise, smoothed things out, or whatever. And that provides useful tools for some of the things we're doing, but that's not the focus of the course. There are courses that do that, I mean 6.003 does some of it already, 6.344, 6.341, there used to be 6.342. So there's a slew of image processing courses that tell you how to program your DSP to do some transformation on an image, and that's not what we're doing. This is not about pattern recognition. So I think of pattern recognition as, you know, you give me an image, and I'll tell you whether it's a poodle or a cat. We're not going to be doing that. And there, of course, there are some courses on that that touch on that in course 9, particularly with respect to human vision and how you might implement those capabilities in hardware. And of course, machine learning is into that.""]"
16,1,16_Course project collaboration,['Course project collaboration'],"['home quizzes', 'problems quizzes', 'quizzes', 'homework problems', 'textbook', 'courses', 'homework', 'home', 'starting', 'problems']","[""And to help you with that, we have like a canned ready-made project that you can modify rather than starting from scratch. Okay, what else? Grades. So, for 6.801, it's a split. Half for your homework problems and half for your take-home quizzes. So, clearly, the take-home quizzes count more. For 6.866, it's split three ways. A third for take-home homework problems, a third for quizzes, and a third for the project. And again, collaboration on the projects. I actually favor it because, you know, there's just a finite length of time in the term. You've got other courses to deal with. Oftentimes, people end up postponing it near the end. So, if you're working with someone else, that can often encourage you to start early and also make sure that you're making some progress. Textbook.""]"
17,1,17_Tomography concepts and challenges,['Tomography concepts and challenges'],"['2d 2d', 'tomography', '2d', 'contrast', 'inversion', 'view', 'colored', 'direction', 'complicated', 'room']","[""It's sort of a curvilinear 2D to 2D. And what's the contrast? Well, suppose I fill the room up with Jell-O. And then somebody goes in and with a hypodermic injects colored dye all over the show. And then I come in the door. And I'm not allowed to move around. I can just stand at the door. And I can look in the room. Can I figure out the distribution of colored dye? No, because in every direction, everything is superimposed from the back of the room to the front. And so you can't disentangle it from one view. Can you do it? Yeah, if you have lots of views. And that's tomography. So we're sort of in an interesting world. Tomography, in a way, is more complicated. But it's also, in a way, much simpler. The math is very simple. And we have a world where there's a match of dimensions. But the equations are complicated. So it's not so easy to do that inversion. Let's see if we need to. I think we need to stop. OK, any questions? So about the homework problem, you should be able to do at least the first three or five questions, probably the fourth. And then on Tuesday, we'll cover what you need to do the last one. You've got to do the last one. OK, that's it for me. I'll see you on Tuesday. Thanks. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye. Bye.""]"

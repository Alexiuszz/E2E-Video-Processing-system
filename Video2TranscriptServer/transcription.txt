 Good morning, everybody. My name's Jason Koo. I'm going to be teaching this class in Introduction to Algorithms with two other instructors here, faculty in the department, Eric DeMain and Justin Solomon.
 They're excellent people and so they will be working on teaching this class with me. I will be teaching the first lecture and we'll have each of them teach one of the next two lectures and then we'll go from there. Yeah, so that's, this is Intro to Algorithms. Okay, so we're gonna start talking about this course content now. What is this course about? It's about algorithms, right?
 in algorithms, really what the course is about is teaching you to solve computational problems. But it's more than that. It's not just about teaching you to solve computational problems. So solve goal one, solve computational problems. But it's more than that. It's also about communicating
 those solutions to others and being able to communicate that your way of solving the problem is correct and efficient. Okay, so it's about two more things. Prove correctness, argue efficiency, and in general,
 In general, it's about communication, I can't spell by the way, communication of these ideas. And you'll find that over the course of this class, you'll be doing a lot more writing than you do in a lot of your other courses. It really should maybe be a CI kind of class because you'll be doing a lot more writing than you will be coding, I'm sure. So it's really, of course, solving the computational
 a problem is important, but really the thing that you're getting out of this class and other theory classes that you're not getting in other classes in this department is that we really concentrate on being able to prove that the things you're doing are correct and better than other things and being able to communicate those ideas to others and not just to a computer, right? So other people convince them that it's correct. Okay, so that's what this class is about.
 What do I mean when I say solve a computational problem? What is a problem? What is an algorithm? I know people make fun of me because I start with this question, but I mean, anyone wanna answer that question? No? What's a problem? Computation. No? Okay, so it's not such a stupid question. Yeah? Something you want to compute, okay?
 Yes, that's true. But kind of a little bit more abstractly, what I'm going to think of a computational problem being, and this is where kind of your prerequisite in discrete mathematics should come in. A problem is kind of you've got a set of inputs. Inputs. OK, maybe I have one, two, three, four, five possible inputs I could have to my algorithm.
 then I have a space of outputs. Outputs. Maybe, I don't know, maybe I have more of them than I do inputs, but these are the possible outputs to my problem. And what a problem is, is a binary relation between these inputs and outputs. Essentially, for each input, I specify which of these outputs is correct. It doesn't necessarily have to be one.
 give me the index in an array containing the value five, there could be multiple fives in that array, and so any of those indices would be correct. So maybe this guy maps to that output, and maybe this guy maps to, I don't know, two or three outputs. This input goes to one, two, I don't know. There's some kind of mapping here. These edges represent a binary relation. It's kind of a graph,
 graph between these inputs and outputs, and these are specifying which of these outputs are correct for these inputs. All right? That's really the formal definition of what a problem is. Now generally, if I have a problem, a computational problem, I'm not going to specify the problem to you by saying, OK, for input one, the correct answer is zero. And for input two, the correct answer is three, and so on and so forth. That would take forever, right?
 Usually what we do when defining a problem is specify some kind of predicate saying that, oh, you can check. If I give you an input and an output, I can check whether that output is correct or not. That's usually how we define a problem is if I am checking for whether this index contains a five, I can just go to that array, look at index five, or the index you gave me, and see if it equals five.
 we're putting it in terms of predicates because in general, we don't really want to talk about small instances of problems. So let's say I had the problem of among the students in this classroom, do any pair of you have the same birthday? All right, well, probably if there's more than 365 of you, the answer is yes. That by what? Pigeonhole principle. Two of you must have the same birthday.
 So let's generalize it a little bit, say that, oh, I don't know, I need a bigger space of birthdays for this question to be interesting. Maybe I tack on the year, maybe I tack on the hour that you were born. And that's a bigger space of inputs, and I wouldn't necessarily expect that two of you would be born in the same year, on the same day, in the same hour. That would be a little less likely. In fact, as long as that space is larger than something like
 the square of the number of you, right, then I'm less likely than even to have a pair of you. That's kind of a birthday problem you may have seen in 042, potentially, right? But in general, I'm not gonna mess with probability so much here. I want a deterministic algorithm, right, a way of checking
 whether two of you have the same birth time, let's say. Okay, so in general, in this class, we're not gonna concentrate on input such as is there a pair of you in this class that have the same birthday? That's kind of boring, right? I could just say, well, I could do a lot of different things, but what we do in this class, this is for a fixed classroom of you, I want to make algorithms
 algorithms that are general to any classroom. To go to your recitation, I want an algorithm that will apply to your recitation. I want an algorithm that not only applies to this classroom, but also the machine learning classroom for you. I want an algorithm that can change, it can accept an arbitrarily sized input. Here we have a class of maybe 300, 400 students, but I want my algorithm to work for a billion students.
 trying to check if there's a match of something in the Facebook data. OK, so in general, we are looking for general problems that have arbitrarily sized inputs. So these inputs could grow very large,
 You just want a kind of a fixed size algorithm to solve those problems. So what is an algorithm then? I really can't stop. I didn't lie to you. So an algorithm is a little different than a problem. A problem specification
 I can state to you what this graph or I can tell you what this graph looks like. An algorithm is really, I don't know what the outputs are, I don't know what these edges are, but I want a fixed size kind of machine or procedure that if I give it an input, it will generate an output, and if it generates an output, it better be one of these correct outputs. So if I have an algorithm that takes in this input, I really want it to
 output this output or else it's not a correct algorithm. Similarly for this one, it could output any of these three outputs, but if it outputs this guy for this input, that would not be a correct algorithm, right? And so generally what we want is an algorithm as a function. It takes inputs to outputs, right? An algorithm is some kind of function, right, that takes these inputs, maps it to a single output, and that output better be
 based on our problem. So that's what our algorithm is. It solves the problem if it returns a correct output for every problem input that is in our domain. And the example for, does anyone have a possible algorithm for checking whether any two of you have the same birth time as specified before? I'm gonna let someone else have a try.
 Sure. Ah, great. So what your colleague has said is a great algorithm. Essentially what it's going to do is I'm going to put you guys in some order. I'm going to give each of you a number, one through however many number of students there are in this class. And I'm going to interview you one by one. I'm going to say, what's your birthday? And then I'm going to write it down. I'm going to put it in some kind of record.
 Okay, and then as I keep interviewing you, I'm gonna find out your birthday, I'm gonna check the record. I'm gonna look through all the birthdays in the record, if I find a match, right? Then I return, yay, I found a pair and I can stop. Otherwise, if I get through the record list, and I don't find a match, I just stick you at the end of the record, right? I add you to the record, and then I move on to the next person. I keep doing this. Okay, so that's a proposed algorithm for this birthday problem.
 for birthday problem. What's the algorithm here? Maintain a record. Interview students, students in some order.
 And what does interviewing a student mean? It means two things. It means check if birthday in record. And if it is, return a pair. So return pair.
 Add new student to record. And then at the very end, if I go through everybody and I haven't found a match yet, I'm gonna return that there is none. Okay, so that's a statement of an algorithm. That's kind of the level of description that we'll be looking for you in the theory parts
 this theory questions that we ask you on your problem sets. It's a verbal description in words that, it's maybe not enough for a computer to know what to do, but if you said this algorithm to any of your friends in this class, they would at least understand what it is that you're doing. Yeah. Pure function, does an algorithm have to be a pure function in mathematics
 in the sense as in it needs to map to a single output. So we're talking about kind of a functional programming definition of a function, right? This is, I am talking about the mathematical, I have a binary relation and this thing has an output for every input.
 and there is exactly one output to every input. That's the mathematical definition of function that I'm using for when I'm defining an algorithm. Yeah. Yeah, an algorithm's a procedure that somehow I can do whatever I want, but I have to take one of these inputs and I have to produce an output and at the end it better be correct. So it's just a procedure. You can think of it as like a recipe
 you can think of it as, it's just some kind of procedure, right? It's a sequence of things that you should do and then at the end you will return. Okay, so here's a possible algorithm for solving this birthday problem. Okay? Now, I've given you what I argue to you, or I'm asserting to you, is a solution to this birthday problem, and maybe you guys agree with me and maybe some of you don't, right? So how do I convince you
 convince you that this is correct, right? Well, if I had, you know, if I was just running this algorithm on, say, the four students in the front row here, right, I could argue it pretty well to you. I could go through every, you know, I could assign these four people birthdays in various combinations of either their, none of them have the same birthday,
 some two of them have the same birthday, I could try all possibilities and I could go through lots of different possibilities and I could just check this algorithm that turns the right answer in all such cases. But when I have, I don't know, 300 of you, that's gonna be a little bit more difficult to argue. And so if I wanna argue something is correct in mathematics, I wanna prove something to you for some large value, what kind of technique do I use to prove something? Induction. And in general,
 we do in this class, what we do as a computer scientist is we write a constant size piece of code that can take on any arbitrarily large size input. If the input can be arbitrarily large, but our code is small, then that code needs to loop or recurse or repeat some of these lines of code in order to just read that output.
 That's another way you can arrive at this conclusion that we're gonna probably need to use recursion, induction, and that's part of the reason why we ask you to take a course on proofs and inductive reasoning and discrete mathematics before this class. Okay, so how do we prove that this thing is correct?
 induction, so how can we set up this induction? What do I need for an inductive proof? Sure. Base case, we need a base case. We need some kind of a predicate, yeah, but we need some kind of statement of a hypothesis of something that should be maintained, right? And then we need to have an inductive step, which basically says I take
 a small value of this thing, I use the inductive hypothesis and I argue it for a larger value of my well-ordered set that I've inducted. Okay, so for this algorithm, if we're gonna try to prove correctness, what I'm gonna do is I'm going to, what do I wanna prove for this thing? That at the end of interviewing all of you, that my algorithm has either already, it has returned with a pair that match,
 Or if we're in a case where there wasn't a mat, wasn't a pair somewhere in my set, then it would turn out. That would be correct. So how can I generalize that concept to make it something I can induct on? What I'm going to do is I'm gonna say, let's say after I've interviewed the first K students, if there was a match in those first K students,
 I want to be sure that I will return the pair. Because if after I interview all of you I've maintained that property, then I'll be sure at the end of the process I will have returned a pair. So here's going to be my inductive hypothesis.
 First k students contain a match. Algorithm returns a match before interviewing.
 say student k plus 1. OK, so that's going to be my inductive hypothesis. Now, if there's n students in this class, and at the end of my thing, I'm trying to interview student n plus 1. Oh, student n plus 1 is not there. If I have maintained this, then if I replace pay with n, then I will have returned a match.
 before interviewing the last student, or when I have no more students left. And then this algorithm returns none, as it should. So this inductive hypothesis sets up a nice variable to induct on. This k I can have increasing up to n, starting at some base case. So what's my base case here?
 My base case is the easiest thing I can do. Two, that's an easy thing I could do. I could check those possibilities, but there's an even easier base case. There's an even easier base case than one. Zero, right? After interviewing zero students, I haven't done any work. Certainly the first zero can't have a match. And so this predicate,
 this inductive hypothesis is true just because this initial predicate is false. So I can say base case zero, check. Definitely this predicate holds for that. Now we get to go for the meat of this thing. Assume the inductive hypothesis
 This is true for k equals, say, some k prime. And we're considering k prime plus 1. Then we have two cases. One of the nice things about induction is that it isolates our problem to not consider everything all at once, but break it down into a smaller interface so I can do less work.
 So there are two cases. Either the first K already had a match, in which case by our inductive hypothesis we've already returned the correct answer. The other case is it doesn't have a match and we interview the K plus 1 student, the K prime plus 1 student.
 And if there is a match in the first k prime plus one students, then it will include k prime plus one, the student k prime plus one, because otherwise there would have been a match in things before it. So there are two cases. If k contains match k prime, if first k contains match,
 already returned by induction. Else, if k prime plus 1 students contains match, the algorithm checks all of the possibilities.
 K prime checks against all students, essentially by brute force. It's a case analysis. I check all of the possibilities, right? This check if birthday is in record. I haven't told you how to do that yet, but if I'm able to do that, I'm going to check if it's in the record. If it's in the record, then there will be a match, and I'm going to return it.
 Otherwise, I have re-established the inductive hypothesis for the k prime. Does that make sense? Yeah. OK, so that's how we prove correctness. This is a little bit more formal than we would ask you to do in this class all the time, but it's definitely sufficient.
 for the levels of arguments that we'll ask you to do. The bar that we're usually trying to set is if you communicated to someone else taking this class what your algorithm was, they would be able to code it up and tell a stupid computer how to do that thing. Okay? So, any questions on induction? You're gonna be using it throughout this class, and so if you're
 unfamiliar with this line of argument, then you should go review some of that. That would be good. OK, so that's correctness, being able to communicate that the problem, the algorithm we stated was correct. Now we want to argue that it's efficient. What does efficiency mean?
 So this basically just means not only how fast does this algorithm run, but how fast does it compare to other possibilities of approaching this problem. So how could we measure how fast an algorithm runs? This is kind of a silly question, yeah? Yeah, just, well, I mean, just record the time it takes for a computer to do this thing.
 with just coding up an algorithm, telling a computer what to do, and timing how long it takes, why? Yeah. It would depend on the size of your data set. Okay, we expect that, but there's a bigger problem there, yeah? It depends on the strength of your computer, right? So I would expect that, you know, if I had a watch calculator and I programmed it to do something,
 That might take a lot longer to solve a problem than if I asked IBM's research computer to solve the same problem using the same algorithm, even with the same code, because its underlying operations are much faster. How it runs is much faster. So I don't want to count how long it would take on a real machine. I kind of want to abstract the time
 it takes the machine to do stuff out of the picture. What I kind of want to say is, let's assume that each kind of fundamental operation the computer can do takes some fixed amount of time. How many of those kinds of fixed operations does the algorithm need to perform to be able to solve this problem? So here we don't measure time.
 instead count kind of fundamental operations. We'll get to what some of those fundamental operations are in a second, but the idea is we want a measure of how well an algorithm performs, not necessarily an implementation of that algorithm. Kind of an abstract notion of how well this algorithm does.
 So what we're going to use to measure time or efficiency is something called asymptotic analysis. Anyone here understand what asymptotic analysis is? Probably, since it's in both of your prerequisites, I think. But we will go through a formal definition of asymptotic notation in recitation tomorrow. And you'll get a lot of practice in comparing functions
 in asymptotic analysis. But just to give you an idea, the idea here is you don't measure time. You instead measure ops. And like your colleague over here was saying before, we expect performance. I'm going to use performance instead of time here. We expect that to depend on performance.
 the size of our input. If we're trying to run an algorithm to find a birthday in this section, we expect the algorithm to run in a shorter amount of time than if I were to run the algorithm on all of them. So we expect it to perform differently depending on the size of the input, and how differently is how we measure performance
 relative to that. Usually we use n as a variable for what the size of our input is, right? But that's not always the case. So for example, if we have an array that I give you, an n by n array, right? That we're gonna say n, but what's the size of our input? How much information do I need to convey to you to give you that information? 10 squared, right? So that's the size of our input in that context, right? Or if I give you a graph, it's usually the number
 the number of vertices plus the number of edges, that's how big, how much space I would need to convey to you that graph, that information. Okay, so you compare how fast an algorithm is with respect to the size of the input, right? And if that, we'll use the asymptotic notation, we have big O notation, which corresponds to upper bounds.
 You'll have omega, which corresponds to lower bounds. And we have theta, which corresponds to both. This thing is tight. It is bounded from above and below by a function of this form.
 Now we have a couple common ways, a couple common functions that algorithms their running time. We have a couple common functions that relate an algorithm's input size to performance. Some things that we saw all the time. Can anyone give me some of those? Say again? Sorry, so like a function.
 Function, I'm not asking this question well, but has anyone heard of a linear algorithm? A linear time algorithm, right? That's basically saying that the running time of my algorithm, performance of my algorithm is linear with respect to the size of my input, right? Yeah. Say again? Like putting something in a list, okay, so that's,
 There's a lot behind that question that we'll go into later this week, but that's an example of if I do it in a silly way, I stick something in the middle of a list and I have to move everything, that's an operation that could take linear time. So linear time is a type of function. We've got a number of these. I'm gonna start with this one. Does anyone know what this one is? Constant time. Basically, no matter how I change the input,
 the amount of time this running time, the performance of my algorithm takes, it doesn't really depend on that. The next one up is something like this. This is logarithmic time. We have theta n, which is linear, and log n. Sometimes we call this log linear, but we usually just say n log n.
 OK? We have a quadratic running time. In general, if I have a constant power up here, it's n to the c for some constant. This is what we call polynomial time, as long as c is some constant. And this right here is what we mean by efficient in this class. In some other classes, when you have big data sets, maybe this is efficient.
 In this class, generally what we mean is polynomial. And as you get down this thing, these things are more and more efficient. There's one class I'm going to talk to you about over here, which is something like let's do this 2 to the theta n. Exponential time, this is some constant to a function of n that's, say, linear.
 that's going to be pretty bad. Why is it pretty bad? If I were to plot some of these things as a function of n, let's say I plot values of up to 1000 on my n scale here. What does constant look like? Maybe this is 1000. What does a constant look like? Looks like a line.
 It looks like a line over here somewhere. It could be as high as I want, but eventually anything that's an increasing function will get bigger than this. And on this scale, if I use log base two or some reasonable small constant, what does log look like? Well, let's see an easier one. What does linear look like? Yeah, this. That's all I'm doing.
 that's the kind of base that we're comparing everything against. What does log look like? Like this, okay. But at this scale, right? At this scale, really, it's much closer to constant than linear. And actually, as n gets much, much larger, this almost looks like a straight line. It almost looks like a constant. So log is almost just as good as constant, right?
 What does exponential look like? It's the exact inverse of this thing. It's almost an exact straight line going up. So this is crap. This is really good. Almost anything in this region over here is better. At least I'm gaining something. I'm able to not go up too high.
 relative to my input size. So quadratic, I don't know, something like this, and n log n is something like this. n log n, after a long time, really starts just looking linear with a constant multiplier. Right? Okay, so these things good, that thing bad, okay? That's what that's trying to do. All right, so how do we measure these things if I don't know what my fundamental operations are that my computer can
 can use, right? So we need to define some kind of model of computation for what our computer is allowed to do in constant time, in a fixed amount of time, right? In general, what we use in this class is a machine called a word ramp, which we use for its theoretical gravity, right?
 Word RAM is kind of a loaded term. What do these things mean? It means I have, does someone know what RAM means? Random access memory, right? It means that I can randomly access different places in memory in constant time. That's the assumption of random access memory. Basically, what our model of a computer is
 We have memory, memory, which is essentially just a string of bits. It's just a bunch of ones and zeros. And we have a computer like a CPU, which is really small. It can basically hold a small amount of information, but it can change that information. It can operate on that information, and it also has instructions to randomly access different places in memory, bring it into the CPU, act on it,
 and read it back. Does that make sense? But in general, we don't have an address for every bit in memory, every zero and one in memory. Does anyone know how modern computers are addressed? Yeah. Okay, so we're gonna get there. Actually, what a modern computer is addressed in
 is bytes, collections of eight bits. So there's an address I have for every eight bits of memory, consecutive eight bits of memory. So if I want to pull something in into the CPU, I give it an address, it'll take some chunk, and bring it into the CPU, operate it on it, and scoot it back. How big is that chunk? This goes to the answer that you were asking, or saying, which is,
 It's some sequence of some fixed number of bits, which we call a word. A word is how big of a chunk that the CPU can take in from memory at a time and operate on. In your computers, how big is that word size? 64 bits. That's how much I can operate on at a time. When I was growing up, when I was your age, my word size was 32 bits.
 That actually was a problem for my computer because in order for me to be able to read to address in memory, I need to be able to store that address in my CPU, in a word. But if I have 32 bits, how many different addresses can I address? I have a limitation on the memory addresses I can address.
 How many addresses can I address with 32 bytes? Two to the 32, right? That makes sense. Well, if you do that calculation out, how big of a hard disk can I have to access? It's about four gigabytes, right? So in my day, all hard drives were limited to being partitioned, even if you had a bigger than four gigabyte hard drive, I had to partition it into these four gigabyte chunks, which the computer could then
 read onto it, right? That was very limiting actually. That's a restriction. With 64 bits, what's my limitation on memory that I can address? Byte addressable. Turns out to be something like 20 exabytes. To put this in context, all data that Google stores on their servers, on all drives throughout the world,
 of the world, it's about 10. So we're not gonna run out of this limitation very soon. So what do we got? We got a CPU, it can address memory. What are the operations I can do in this CPU? Generally, I have binary operations. I can compare two words in memory, and I can either do integer arithmetic,
 kind of logical operations, bitwise operations. We're not gonna use those so much in this class. And I can read and write from an address in memory, a word, in constant time. Those are the operations that I have available to me on most CPUs. Some CPUs give you a little bit more power, but this is generally
 what we analyze algorithms with respect to, okay? But you'll notice that my CPU is only built to operate on a constant amount of information at once. Generally, two words in memory, an operation produces a third one and I spit it out, right? It takes a constant amount of time to operate on a constant amount of memory. If I want to operate on a linear amount of memory,
 How long is that gonna take? I just want to read everything in that thing. It's gonna take me linear time, right? Because I have to read every part of that thing. Okay, so in general, what we're gonna do for the first half of this class mostly, first eight lectures anyway, is talk about data structures. And it's going to be concerned about
 it's not operating on constant amount of data at a time like our CPU is doing, but instead what it's going to do is operate on, store a large amount of data and support different operations on that data. So if I had a record that I want to maintain to store those birthdays that we had before, I might use something like a static array, which you guys maybe are not familiar with,
 you have been working in Python as your only programming unit. Python has a lot of really interesting data structures like a list and a set and a dictionary and all these kinds of things that are actually not in this model. There's actually a lot of code between you and the computer and it's not always clear how much time that interface is taking. And so what we're gonna do starting on Thursday is talk about ways of
 of storing a non-constant amount of information to make operations on that information faster. So just before you go, I just want to give you a quick overview of the class. To solve an algorithm problem in this class, we essentially have two different strategies. We can either reduce to using the solution to a problem we know how to solve, or we can design our own algorithm, which is going to be recursive in nature.
 We're gonna either put stuff in the data structure, and then solve a sorting problem, or search in a graph. And then to design a recursive algorithm, we have various design paradigms. This is all in your notes, but this is essentially the structure of the class. We're gonna spend quiz one, the first eight lectures, on data structures and sorting. Second quiz will be on shortest paths, algorithms, and graphs. And then the last one will be on dynamic programming. Okay, that's the end of
 of the first lecture. Thanks for coming.